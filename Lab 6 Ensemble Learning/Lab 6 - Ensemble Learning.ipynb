{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Zewail City of Science and Technology</h1>\n",
    "<h2 align=\"center\">CIE 417 (Fall 2020)</h2>\n",
    "<h2 align=\"center\">Lab 6: Ensemble Methods </h2>\n",
    "<h3 align=\"center\">Eng. Ahmed Wael</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Objectives:\n",
    "\n",
    "- Understanding the idea of Ensemble Learning\n",
    "\n",
    "\n",
    "- Understanding the idea of Bootstraping\n",
    "\n",
    "\n",
    "- Understanding the idea of Bagging\n",
    "\n",
    "\n",
    "- What do we mean by Out-Of-Bag (OOB) Error\n",
    "\n",
    "\n",
    "- Implementing Random Forests from scratch to understand the last three points better\n",
    "\n",
    "\n",
    "- Discuss the different hyberparamaters of the RF\n",
    "\n",
    "\n",
    "- Understaing the idea of Boosting\n",
    "\n",
    "\n",
    "- Discuss AdaBoost, one of the most famous Boosting Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from math import log2, sqrt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import six\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from decision_tree import MyDecisionTree ## Our implementation from lab 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Dataset : \n",
    "We are going you use the same dataset from the last lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>state_sample</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>c3</th>\n",
       "      <th>eligible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>c14</td>\n",
       "      <td>c21</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>c337</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>c14</td>\n",
       "      <td>c21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>c337</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>c14</td>\n",
       "      <td>c21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>c337</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>c12</td>\n",
       "      <td>c21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>c337</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>c12</td>\n",
       "      <td>c20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>c34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  state_sample   education  education_num  \\\n",
       "0   39          State-gov         77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc         83311   Bachelors             13   \n",
       "2   38            Private        215646     HS-grad              9   \n",
       "3   53            Private        234721        11th              7   \n",
       "4   28            Private        338409   Bachelors             13   \n",
       "\n",
       "        marital_status          occupation    relationship   c1   c2  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family  c14  c21   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband  c14  c21   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family  c14  c21   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband  c12  c21   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife  c12  c20   \n",
       "\n",
       "   capital_gain  capital_loss  hours_per_week    c3  eligible  \n",
       "0          2174             0              40  c337         0  \n",
       "1             0             0              13  c337         0  \n",
       "2             0             0              40  c337         0  \n",
       "3             0             0              40  c337         0  \n",
       "4             0             0              40   c34         0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"adult_data.csv\")\n",
    "\n",
    "categorical = ['workclass', 'education', 'marital_status', 'occupation', \n",
    "                   'relationship', 'c1', 'c2', 'c3']\n",
    "numerical = ['age', 'education_num','capital_gain', 'capital_loss',\n",
    "                'hours_per_week']\n",
    " \n",
    "X = pd.concat([data[categorical], data[numerical]], axis=1) ## Just sort them, no need for encoding\n",
    "y = data['eligible']\n",
    "X_train , X_test , y_train , y_test = train_test_split(X,y,test_size = 0.2 , random_state = 10)\n",
    "X_train , X_test , y_train , y_test = X_train.to_numpy() , X_test.to_numpy() , y_train.to_numpy() , y_test.to_numpy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Ensemble Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Combine many mediocre (weak) models in a one super model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning [Example](https://www.analyticsvidhya.com/blog/2015/08/introduction-ensemble-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I want to invest in some company, but I am not sure about its performance though. \n",
    "\n",
    "\n",
    "- So, I look for advice on whether the stock price will increase more than 6% per annum or not? \n",
    "\n",
    "\n",
    "- I decide to approach various experts having **diverse** domain experience:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Employee of the Company:** \n",
    "    - *Good At :*\n",
    "    \n",
    "        - Knows the internal functionality of the company\n",
    "        \n",
    "        - Has the insider information about the functionality of the firm.\n",
    "    \n",
    "    - *Bad At :*\n",
    "        - Lacks a broader perspective on how are competitors innovating.\n",
    "   \n",
    "        - Lacks on how is the technology evolving and what will be the impact of this evolution on the Company's product. \n",
    "   \n",
    "    - *Accuracy:*\n",
    "        - In the past, he has been right 70% times.\n",
    "\n",
    "\n",
    "2. **Financial Advisor of the Company:** \n",
    "    - *Good At :*\n",
    "    \n",
    "        - Has a broader perspective on how companies strategy will fair of in this competitive environment.\n",
    "            \n",
    "    - *Bad At :*\n",
    "        - Lacks a view on how the company’s internal policies are fairing off\n",
    "        \n",
    "    - *Accuracy:*\n",
    "        - In the past, he has been right 75% times.\n",
    "\n",
    "\n",
    "3. **Stock Market Trader:**\n",
    "    - *Good At :*\n",
    "    \n",
    "        - Observed the company’s stock price over past 3 years.\n",
    "        - Knows the seasonality trends and how the overall market is performing.\n",
    "        - Developed a strong intuition on how stocks might vary over time.\n",
    "    - *Bad At :*\n",
    "        - Doesn't now the internal of the company.\n",
    "        \n",
    "    - *Accuracy:*\n",
    "        - In the past, he has been right 70% times.\n",
    "\n",
    "\n",
    "4. **Employee of a competitor:**\n",
    "    - *Good At :*\n",
    "    \n",
    "        - Knows the internal functionality of the competitor firms\n",
    "        - Aware of certain changes which are yet to be brought.\n",
    "    - *Bad At :*\n",
    "        - Lacks a sight of company in focus and the external factors which can relate the growth of competitor with the company of subject.\n",
    "        \n",
    "    - *Accuracy:*\n",
    "        - In the past, he has been right 60% times.\n",
    "\n",
    "\n",
    "5. **Market Research team in same segment:**\n",
    "    - *Good At :*\n",
    "    \n",
    "        - analyzes the customer preference of the company's product over others and how is this changing with time.\n",
    "        - Aware of certain changes which are yet to be brought.\n",
    "    - *Bad At :*\n",
    "        - Unaware of the changes the company will bring because of alignment to its own goals, because he deals with customer side.\n",
    "        \n",
    "    - *Accuracy:*\n",
    "        - In the past, he has been right 75% times.\n",
    "\n",
    "\n",
    "\n",
    "6. **Social Media Expert:**\n",
    "    - *Good At :*\n",
    "    \n",
    "        - Can help us understand how the company have positioned its products in the market\n",
    "        - Can help us understand how are the sentiment of customers changing over time towards company.\n",
    "    - *Bad At :*\n",
    "        - Unaware of any kind of details beyond digital marketing.\n",
    "        \n",
    "    - *Accuracy:*\n",
    "        - In the past, he has been right 65% times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the accuracy of the 6 experts combined?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$1 - \\prod Error(expert) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$1 - (30\\% \\cdot 25\\% \\cdot 30\\% \\cdot 40\\% \\cdot 25\\% \\cdot 35\\%) = 1 - 0.07875 = 99.92125\\% $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the problem in this approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We assume that all the predictions are **completely** independent on each other. Which doesn't make sense and not realistic at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to solve this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Bagging:** **Parallel** training of **different** models using **overlapping** training sets. Also called *Bootstrap AGgregation* \n",
    "\n",
    "\n",
    "- **Boosting:** **Sequential** training by iteratively **re-weighting** training examples such that each model focuses on a different subset of **hard examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap Sampling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does it address?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bootstrap address a very famous challenge in statistics.\n",
    "\n",
    "\n",
    "- We only have sample of a population, but we want to collect information about the WHOLE population.\n",
    "\n",
    "\n",
    "- How to make sure that this sample represent the whole population? It the mean of the sample is a good-enough approximation for the mean of the population?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of estimating the statistic - mean for example - only once, we do this many times.\n",
    "\n",
    "\n",
    "- We do it using re-sampling **with replacement** of the original sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://miro.medium.com/max/875/1*iH5w0MBdiOlxDOCX6nmqqw.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://miro.medium.com/max/1250/1*y42mIxqot08pZq8_ZoFiXw.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging - *Bootstrap Aggregation* - :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its basic idea is to create different models using **different bootstrapped**\n",
    "training sets and **average** their prediction to produce the final output\n",
    "of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is it good for?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reducting the variance of the model.\n",
    "\n",
    "\n",
    "- Thus, reducing overfitting\n",
    "\n",
    "\n",
    "- Outliers are likely omitted in some of the training bootstap samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://hudsonthames.org/wp-content/uploads/2019/09/bagging-1.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate B different bootstrapped training data sets. \n",
    "\n",
    "\n",
    "2. Train our ML method on the *b* bootstrapped training set\n",
    "\n",
    "\n",
    "3. $$\\hat{f}(x) = \\frac{1}{B}\\sum\\hat{f}^b(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Calculate the test error in Bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By using the **Out-of-Bag Error (OOB) :**  The mean prediction error on each training sample *x*, using only the model that did not have *x* in their bootstrapped training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- No need to use cross-validation or hold-out samples in order to get an unbiased error estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Because, in Bagging, the error estimation takes place internally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = https://miro.medium.com/max/875/1*HojoYXBU_DTnMuCatCnQaQ.png>\n",
    "\n",
    "                Training                                              Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the OOB here?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have 15 points in the test set, that are NOT in the training set, right?\n",
    "\n",
    "\n",
    "- The model classified 4 of these 15 incorrectly. \n",
    "\n",
    "\n",
    "- So, the accuracy is 11/15*100 = 73.33%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thus, we can say that the OOB estimate is nothing more than the mean estimate of the base algorithms on those ~37% of inputs that were left out of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Importance Measures:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagging improves prediction accuracy at the expense on interpretability.\n",
    "\n",
    "\n",
    "- For example, if we use it with decision trees, it is no longer clear which features are the most important.\n",
    "\n",
    "\n",
    "- So, we calculate an overall summary of importance using Variable Importance Measures such as GINI index and RSS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Random Forest is slightly modified Bagging algorithm.\n",
    "\n",
    "\n",
    "It is basically a collection of **decorrelated** decision trees, built as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For every tree: \n",
    "\n",
    "    1. We subsample the examples with replacement **Bootstraping**\n",
    "\n",
    "    2. For each subsample, we choose attributes **at random**. We randomly pick features in each split. WHY?\n",
    "    \n",
    "    3. We build the tree ONLY on these random features and with the current subsample.\n",
    "\n",
    "    \n",
    "- Classification for a random forest is then done by taking a majority vote of the classifications yielded by each tree in the forest after it classifies an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "    def __init__(self, n_estimators=50, max_depth=None, max_features=0.7):\n",
    "        self.n_estimators = n_estimators ## how many estimators we are going to use?\n",
    "        self.max_depth = max_depth ## Max depth \n",
    "        self.max_features = max_features ## the percentage of the RANDOM features we are going to use\n",
    "        self.bootstraps_row_indices = [] ## Which examples are in the current bootstrap\n",
    "        self.feature_indices = [] ## Keeping track of features indices\n",
    "        self.out_of_bag = [] ## To calculate the out of bag error\n",
    "        \n",
    "        decision_trees = []\n",
    "        for i in range(n_estimators):\n",
    "            decision_trees.append(MyDecisionTree(max_depth=max_depth)) ## Call our DT class\n",
    "        \n",
    "        self.decision_trees = decision_trees ## Our random forest\n",
    "        \n",
    "        ## OR  self.decision_trees = [MyDecisionTree(max_depth=max_depth) for i in range(n_estimators)]\n",
    "        \n",
    "    def _bootstrapping(self, num_training, num_features): ## _ is similar to private in other programming language\n",
    "        \"\"\"\n",
    "        - INPUT : \n",
    "            num_training: how many training examples in this bootstrap\n",
    "            num_features: how many features in this bootstrap\n",
    "            \n",
    "        - OUTPUT :\n",
    "        - row_idx: the row indices corresponding to the row locations of the selected samples in the original dataset.\n",
    "        - col_idx: the column indices corresponding to the column locations of the selected features  \n",
    "                   in the original feature list.\n",
    "        \n",
    "        \n",
    "        \n",
    "        - Randomly select a sample dataset of size num_training with replacement from the original dataset. \n",
    "        - Randomly select certain number of features (num_features denotes the total number of features in X,\n",
    "          without replacement from the total number of features.\n",
    "        \"\"\" \n",
    "        \n",
    "        sample_size = list(range(num_training))\n",
    "        row_idx = np.random.choice(sample_size,num_training) # Create random row indices\n",
    "        col_idx = np.random.permutation(num_features)[:int(num_features*self.max_features)] ## Permutation ( with replacement)\n",
    "        return row_idx, col_idx\n",
    "            \n",
    "    def bootstrapping(self, num_training, num_features):\n",
    "        \"\"\"\n",
    "        Initializing the bootstap datasets for each tree\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            total = set(list(range(num_training))) ## we use a set to get the unique elements\n",
    "            row_idx, col_idx = self._bootstrapping(num_training, num_features)\n",
    "            total = total - set(row_idx) ## again we use a set. Subtract the row indices from the total\n",
    "            self.bootstraps_row_indices.append(row_idx)\n",
    "            self.feature_indices.append(col_idx)\n",
    "            self.out_of_bag.append(total) ## total is used for the OOB \n",
    "            \n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train decision trees using the bootstrapped datasets.\n",
    "        \"\"\"\n",
    "        \n",
    "        num_training, num_features = X.shape\n",
    "        self.bootstrapping(num_training,num_features) ## initialize the bootstrapping\n",
    "        for i in range((self.n_estimators)): ## loop over the trees \n",
    "            current_bootstraps_row_indices = self.bootstraps_row_indices[i]\n",
    "            current_feature_indices = self.feature_indices[i]\n",
    "            current_X = X_train[current_bootstraps_row_indices[:,np.newaxis], current_feature_indices] ## data for this tree\n",
    "            current_y = y[current_bootstraps_row_indices]\n",
    "            current_dt = self.decision_trees[i]\n",
    "            current_dt.fit(current_X,current_y, 0) ## 0 for the initial depth\n",
    "            print(\"Current Tree to fit : \" ,i+1) ## Which tree we are using\n",
    "            \n",
    "            \n",
    "    def OOB_score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate the OOB score\n",
    "        \"\"\"\n",
    "        \n",
    "        accuracy = []\n",
    "        for i in range(len(X)): ## Loop over the full dataset\n",
    "            predictions = []\n",
    "            for t in range(self.n_estimators): ## loop over each decision tree\n",
    "                if i in self.out_of_bag[t]: ## the data that is NOT used in the current tree\n",
    "                    predictions.append(self.decision_trees[t].predict(X[i][self.feature_indices[t]])) ## Predict\n",
    "            if len(predictions) > 0:\n",
    "                accuracy.append(np.sum(predictions == y[i]) / float(len(predictions))) ## Majority voting\n",
    "        return np.mean(accuracy) ## Total accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Tree to fit :  1\n",
      "Current Tree to fit :  2\n",
      "Current Tree to fit :  3\n",
      "Current Tree to fit :  4\n",
      "Current Tree to fit :  5\n",
      "Current Tree to fit :  6\n",
      "Current Tree to fit :  7\n",
      "Current Tree to fit :  8\n",
      "Current Tree to fit :  9\n",
      "Current Tree to fit :  10\n",
      "Current Tree to fit :  11\n",
      "Current Tree to fit :  12\n",
      "Current Tree to fit :  13\n",
      "Current Tree to fit :  14\n",
      "Current Tree to fit :  15\n",
      "Current Tree to fit :  16\n",
      "Current Tree to fit :  17\n",
      "Current Tree to fit :  18\n",
      "Current Tree to fit :  19\n",
      "Current Tree to fit :  20\n",
      "Current Tree to fit :  21\n",
      "Current Tree to fit :  22\n",
      "Current Tree to fit :  23\n",
      "Current Tree to fit :  24\n",
      "Current Tree to fit :  25\n",
      "Current Tree to fit :  26\n",
      "Current Tree to fit :  27\n",
      "Current Tree to fit :  28\n",
      "Current Tree to fit :  29\n",
      "Current Tree to fit :  30\n",
      "accuracy: 0.7785\n"
     ]
    }
   ],
   "source": [
    "n_estimators = 30\n",
    "max_depth = 5\n",
    "max_features = 0.7\n",
    "\n",
    "random_forest = RandomForest(n_estimators, max_depth, max_features)\n",
    "\n",
    "random_forest.fit(X_train, y_train)\n",
    "accuracy=random_forest.OOB_score(X_test, y_test)\n",
    "\n",
    "print(\"accuracy: %.4f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare out implementation with Sklearn Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8239681750372949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:540: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\Ahmed\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:545: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    }
   ],
   "source": [
    "X_train , X_test , y_train , y_test = train_test_split(X,y,test_size = 0.2 , random_state = 10)\n",
    "\n",
    "X_train = pd.get_dummies(X_train,drop_first=True)\n",
    "X_test = pd.get_dummies(X_test,drop_first=True)\n",
    "clf = RandomForestClassifier(max_depth=5, n_estimators = 10 , criterion = 'entropy', oob_score = True)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Pros and Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Pros | Cons\n",
    "| --- | --- \n",
    "|Robust to outliers| biased while dealing with categorical variables.\n",
    "|Works well with non-linear data.|Slow Training.\n",
    "|Lower risk of overfitting.|Not suitable for linear methods with a lot of sparse features\n",
    "|Runs efficiently on a large dataset.|Greedy algorithms don’t yield the global optimum tree structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = https://quantdare.com/wp-content/uploads/2016/04/bb3.png>\n",
    "<img src = https://miro.medium.com/max/875/1*wpVgt07J_TeH3jEdc3A50g.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost - *ADAptive Boost* :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combining multiple weak classifiers to build one strong classifier\n",
    "\n",
    "\n",
    "- with each one progressively learning from the others' wrongly classified objects\n",
    "\n",
    "\n",
    "- Rather than being a model in itself, AdaBoost can be applied on top of any classifier to learn from its shortcomings and propose a more accurate model.\n",
    "\n",
    "\n",
    "- Therefore, it is called *best out-of-the-box classifier*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we mean by a Weak Classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A one that performs better than random guessing, but still performs poorly at designating classes to objects.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For example, a weak classifier may predict that everyone above the age of 40 could not run a marathon but people falling below that age could.\n",
    "\n",
    "\n",
    "- You might get above 60% accuracy, but you would still be misclassifying a lot of data points!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trees with just **one** node and two leaves.\n",
    "\n",
    "\n",
    "- A stump can only use one variable to make a decision\n",
    "\n",
    "\n",
    "- AdaBoost uses a forest of them instead of trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How AdaBoost works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A weak classifier (e.g. a decision stump) is made on top of the training data based on the **weighted samples**.\n",
    "\n",
    "- where each sample weight indicates how important it is to be correctly classified.\n",
    "\n",
    "\n",
    "- Initially, for the first stump, we give all the samples equal weights.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "2. We create a decision stump for each variable and see how well each stump classifies samples to their target classes.\n",
    "\n",
    "\n",
    "- For example, in the diagram below we check for Age, Eating Junk Food, and Exercise. We'd look at how many samples are correctly or incorrectly classified as Fit or Unfit for each individual stump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://lh3.googleusercontent.com/kpQjxgGIxnSnMm495bDs0OZf4rE08E58PV1wwK9q10b_pL5AtKkRcY0OY5Hc_NFY0aW6iRQYAQDKuueEwnOfcEz9_IYyO-Ej-HwAqoFS_rQ779mP5HTHPKCy4x-lBmr33dd-Nw>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. More weight is assigned to the incorrectly classified samples so that they're classified correctly in the next decision stump.\n",
    "\n",
    "\n",
    "- Weight is also assigned to each classifier based on the accuracy of the classifier, which means high accuracy = high weight!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Reiterate from Step 2 until all the data points have been correctly classified, or the maximum iteration level has been reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = https://blog.paperspace.com/content/images/2019/12/WhatsApp-Image-2019-12-30-at-11.55.02-AM.jpeg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\alpha_t = \\frac{1}{2}\\ln\\frac{1-TotalError}{TotalError}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alpha is how much influence this stump will have in the final classification. \n",
    "\n",
    "\n",
    "- Total Error is nothing but the total number of misclassifications for that training set divided by the training set size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = https://i.ibb.co/J5ck9X9/adaboost-alphacurve.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When alpha is zero, the error rate is equal to the random chance - 0.5 - \n",
    "\n",
    "\n",
    "- When alpha is a positive number, the error rate decreases, which means we have a perfect decision stump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w_i = w_{i-1} \\cdot e^{\\pm\\alpha}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Alpha is positive when the predicted and the actual output agree (the sample was classified correctly). In this case we **decrease** the sample weight from what it was before, since we're already performing well.\n",
    "\n",
    "\n",
    "- Alpha is negative when the predicted output does not agree with the actual class (i.e. the sample is misclassified). In this case we need to **increase** the sample weight so that the same misclassification does not repeat in the next stump. This is how the stumps are dependent on their predecessors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Pseduocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initially set uniform example weights.\n",
    "\n",
    "\n",
    "\n",
    "- for Each base learner do:\n",
    "    1. Train base learner with a weighted sample.\n",
    "    2. Test base learner on all data.\n",
    "    3. Set learner weight with a weighted error.\n",
    "    4. Set example weights based on ensemble predictions.\n",
    "- end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging vs Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagging is easy to parallelize and hence training is faster\n",
    "\n",
    "\n",
    "\n",
    "- Boosting is more efficient for fixed no of iterations (classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://www.datacamp.com/community/tutorials/ensemble-learning-python\n",
    "\n",
    "\n",
    "2. https://blog.paperspace.com/tag/series-ensemble-methods/\n",
    "\n",
    "\n",
    "3. https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-5-ensembles-of-algorithms-and-random-forest-8e05246cbba7\n",
    "\n",
    "\n",
    "4. https://towardsdatascience.com/an-introduction-to-the-bootstrap-method-58bcb51b4d60\n",
    "\n",
    "\n",
    "5. https://datasciencechalktalk.com/2019/11/12/bootstrap-sampling-an-implementation-with-python/\n",
    "\n",
    "\n",
    "6. https://blog.paperspace.com/adaboost-optimizer/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 points: Work with the [Vehicle regression dataset](https://www.kaggle.com/nehalbirla/vehicle-dataset-from-cardekho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset, perform all necessary data exploration and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Points: Use Sklearn Random Forest Regressor as the ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform proper hyperparameter tuning to choose the best base model, depth, max_features, and any other important hyper-parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Points: Use Sklearn AdaBoost on the same dataset and compare the results with the RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Task ( 3 Points ) : Use another Boosting Algorithm, such as XGBoost, on the same dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You need to explain in details how the algorithm work and how it is different from AdaBoost in order to receive any credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
